{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWOQCYLSCtb3"
      },
      "outputs": [],
      "source": [
        "# 1. Install the required packages\n",
        "# On Windows, you just need to execute this cell for once.\n",
        "try:\n",
        "    import google.colab\n",
        "    # IN_COLAB = True\n",
        "except ImportError:\n",
        "    # IN_COLAB = False\n",
        "    %pip install -q git+https://github.com/huggingface/transformers\n",
        "    %pip install -q git+https://github.com/huggingface/accelerate\n",
        "\n",
        "%pip install -q git+https://github.com/huggingface/diffusers\n",
        "%pip install -q gradio ftfy tensorboard\n",
        "%pip install -q bitsandbytes\n",
        "%pip install -q xformers --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# Install ipyfilechooser (if not already installed)\n",
        "%pip -q install ipyfilechooser\n",
        "print(\"Package installation finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ggcAoj9sDz6I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lpw_stable_diffusion.py already exists, skipping download.\n",
            "['stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
            " 'Lykon/DreamShaper',\n",
            " 'digiplay/DarkSushi2.5D_v1',\n",
            " 'Dreamlike photoreal',\n",
            " 'deliberate_v11']\n",
            "['https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.safetensors',\n",
            " 'https://huggingface.co/Lykon/DreamShaper/blob/main/DreamShaper_6.2_BakedVae_pruned.safetensors',\n",
            " 'https://huggingface.co/digiplay/DarkSushi2.5D_v1/blob/main/darkSushi25D25D_v10.safetensors',\n",
            " 'https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/blob/main/dreamlike-photoreal-2.0.safetensors',\n",
            " './models\\\\deliberate_v11.safetensors']\n",
            "['php', 'sks']\n"
          ]
        }
      ],
      "source": [
        "# 2. Create folders and download training scripts\n",
        "import os, shutil\n",
        "from pprint import pprint\n",
        "\n",
        "model_dir = \"./models\"\n",
        "embedding_dir = \"./embeddings\"\n",
        "output_dir = \"./output\"\n",
        "\n",
        "# Global variables\n",
        "models = [\n",
        "   \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
        "   \"Lykon/DreamShaper\",\n",
        "   \"digiplay/DarkSushi2.5D_v1\",\n",
        "   \"Dreamlike photoreal\",\n",
        "]\n",
        "\n",
        "links = [\n",
        "   \"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.safetensors\",\n",
        "   \"https://huggingface.co/Lykon/DreamShaper/blob/main/DreamShaper_6.2_BakedVae_pruned.safetensors\",\n",
        "   \"https://huggingface.co/digiplay/DarkSushi2.5D_v1/blob/main/darkSushi25D25D_v10.safetensors\",\n",
        "   \"https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/blob/main/dreamlike-photoreal-2.0.safetensors\",\n",
        "]\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "# browse the model_dir directory and add found items to models and links\n",
        "# browse the embedding_dir and add found items to embeddings\n",
        "\n",
        "pipeline = None\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(embedding_dir, exist_ok=True)\n",
        "# Delete the 'output' folder and its contents\n",
        "shutil.rmtree(output_dir, ignore_errors=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def download_file(filename, url):\n",
        "  # fetch train_dreambooth.py if it doesn't exist\n",
        "  if not os.path.exists(filename):\n",
        "      !wget \"{url}\"\n",
        "  else:\n",
        "      print(f\"{filename} already exists, skipping download.\")\n",
        "\n",
        "download_file(\"lpw_stable_diffusion.py\", \"https://raw.githubusercontent.com/jomo0825/MrFuGenerativeAI/main/DiffusionModel/lpw_stable_diffusion.py\")\n",
        "\n",
        "def update_descriptors():\n",
        "   # Browse the model_dir directory and add found items to models and links\n",
        "   for filename in os.listdir(model_dir):\n",
        "      if filename.endswith('.safetensors'):\n",
        "         model_name = os.path.splitext(filename)[0]\n",
        "         if model_name not in models:\n",
        "            models.append(model_name)\n",
        "         file_path = os.path.join(model_dir, filename)\n",
        "         if file_path not in links:\n",
        "            links.append(file_path)\n",
        "\n",
        "   # Browse the embedding_dir and add found items to embeddings\n",
        "   for filename in os.listdir(embedding_dir):\n",
        "      if filename.endswith('.safetensors'):  # Assuming embeddings are .pt files\n",
        "         embedding_name = os.path.splitext(filename)[0]\n",
        "         if embedding_name not in embeddings:\n",
        "            embeddings.append(embedding_name)\n",
        "\n",
        "   # (Optional) Print the updated lists to verify\n",
        "   pprint(models)\n",
        "   pprint(links)\n",
        "   pprint(embeddings)\n",
        "\n",
        "update_descriptors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qtwnYU8kDbvv"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6ea29e4fc074334b4161334810eb5e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileChooser(path='C:\\work\\github\\MrFuGenerativeAI\\DiffusionModel', filename='', title='<b>Select a <span style…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ec38a60221049ff8a69c9572d06ce9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(description='Upload Model', style=ButtonStyle())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6aa296dde6d842778281c4d352aaf54d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileChooser(path='C:\\work\\github\\MrFuGenerativeAI\\DiffusionModel', filename='', title='<b>Select a <span style…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0612024e4eb49cb9393ccc6f30b80af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(description='Upload Embedding', style=ButtonStyle())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "381b10ff25984d59a6acaacf4ec5a94c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 3. Upload Dreambooth and TI models\n",
        "import os\n",
        "import shutil\n",
        "from ipyfilechooser import FileChooser\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Define a function to move the selected file\n",
        "def move_file(target_directory, file_path):\n",
        "  if file_path is not None:\n",
        "      filename = os.path.basename(file_path)\n",
        "      destination_path = os.path.join(target_directory, filename)\n",
        "      # test if the destination_path alread exist\n",
        "      if not os.path.exists(destination_path):\n",
        "        shutil.copy(file_path, destination_path)\n",
        "        print(f\"Copy {filename} to {target_directory}\")\n",
        "      else:\n",
        "        print(f\"{filename} already exists.\")\n",
        "  else:\n",
        "      print(\"No file selected.\")\n",
        "\n",
        "def upload_model(b):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        move_file(model_dir, chooserDB.selected)\n",
        "        # add_DB(os.path.basename(chooserDB.selected))\n",
        "        update_descriptors()\n",
        "\n",
        "def upload_TI(b):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        move_file(embedding_dir, chooserTI.selected)\n",
        "        # add_TI(os.path.basename(chooserTI.selected))\n",
        "        update_descriptors()\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Define source and target directories\n",
        "    source_directory_DB = '/content/drive/MyDrive/Dreambooth'\n",
        "    source_directory_TI = '/content/drive/MyDrive/TextualInversion'\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    source_directory_DB = \"./\"\n",
        "    source_directory_TI = \"./\"\n",
        "\n",
        "target_directory = model_dir  # Ensure model_dir is defined\n",
        "\n",
        "# Create and display the file chooser widget\n",
        "chooserDB = FileChooser(source_directory_DB)\n",
        "chooserDB.title = '<b>Select a <span style=\"color:red\">Dreambooth</span> model to upload:</b>'\n",
        "display(chooserDB)\n",
        "# Add a button to trigger the file transfer\n",
        "DB_button = widgets.Button(description=\"Upload Model\")\n",
        "DB_button.on_click(upload_model)\n",
        "display(DB_button)\n",
        "\n",
        "# Create and display the file chooser widget\n",
        "chooserTI = FileChooser(source_directory_TI)\n",
        "chooserTI.title = '<b>Select a <span style=\"color:blue\">Textual Inversion</span> embedding to upload:</b>'\n",
        "display(chooserTI)\n",
        "TI_button = widgets.Button(description=\"Upload Embedding\")\n",
        "TI_button.on_click(upload_TI)\n",
        "display(TI_button)\n",
        "output = widgets.Output()\n",
        "display(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GaGpmeILXSGl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A matching Triton is not available, some optimizations will not be enabled\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\anaconda3\\envs\\diffusion\\lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
            "    import triton  # noqa\n",
            "ModuleNotFoundError: No module named 'triton'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef1f57149ff34893a2c00e8300345ad6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b3e2d02a57e40ee913376e77ed94673",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings\\php.safetensors\n",
            "embeddings\\sks.safetensors\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4. Open WebUI\n",
        "import torch\n",
        "import random\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
        "from diffusers import DPMSolverMultistepScheduler, EulerDiscreteScheduler,EulerAncestralDiscreteScheduler\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from lpw_stable_diffusion import StableDiffusionLongPromptWeightingPipeline\n",
        "\n",
        "def load_pipeline(model_name, progress=gr.Progress()):\n",
        "    progress(0, desc=\"Loading text2img pipeline...\")\n",
        "    # In this project we only use .safetensors model file. No pretrained model folders.\n",
        "    # pipe_txt2img = StableDiffusionPipeline.from_single_file(\n",
        "    pipe_txt2img = StableDiffusionLongPromptWeightingPipeline.from_single_file(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
        "        use_safetensors=True\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    pipe_txt2img.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe_txt2img.scheduler.config)\n",
        "\n",
        "    progress(0.5, desc=\"Loading embeddings...\")\n",
        "    for embedding in embeddings:\n",
        "        path = os.path.join(embedding_dir, f\"{embedding}.safetensors\")\n",
        "        path = os.path.normpath(path)\n",
        "        print(path)\n",
        "        pipe_txt2img.load_textual_inversion(path)\n",
        "\n",
        "\n",
        "    # pipe_img2img = StableDiffusionImg2ImgPipeline(\n",
        "    #     vae=pipe_txt2img.vae,\n",
        "    #     text_encoder=pipe_txt2img.text_encoder,\n",
        "    #     tokenizer=pipe_txt2img.tokenizer,\n",
        "    #     unet=pipe_txt2img.unet,\n",
        "    #     scheduler=pipe_txt2img.scheduler,\n",
        "    #     safety_checker=pipe_txt2img.safety_checker,  # This will get the safety checker from txt2img\n",
        "    #     feature_extractor=pipe_txt2img.feature_extractor,  # This will get the feature extractor from txt2img\n",
        "    # ).to(\"cuda\")\n",
        "    # pipe_img2img.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe_img2img.scheduler.config)\n",
        "    progress(1.0, desc=\"Done!\")\n",
        "    return pipe_txt2img\n",
        "\n",
        "def txt2img(prompt, negative_prompt, height, width, steps, guidance_scale, seed):\n",
        "    global pipeline\n",
        "    if seed == -1:\n",
        "        seed = random.randint(0, 2**32 - 1)\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "    output = pipeline(\n",
        "        prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        num_inference_steps=steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=generator\n",
        "    )\n",
        "\n",
        "    if not output.nsfw_content_detected[0]:\n",
        "      #print(\"Not NSFW\")\n",
        "      return output.images[0], \"Success\", f\"Noise seed: {seed}\"\n",
        "    else:\n",
        "      #print(\"NSFW Detected\")\n",
        "      return output.images[0], \"NSFW content detected\", \"NSFW content detected\"\n",
        "\n",
        "    #if len(output.images) == 0:\n",
        "    #   return None, \"NSFW content detected\"\n",
        "    #return output.images[0], \"Success\"\n",
        "\n",
        "def img2img(init_image, prompt, negative_prompt, height, width, strength, steps, guidance_scale, seed):\n",
        "    global pipeline\n",
        "    if init_image is None:\n",
        "        return None\n",
        "    if seed == -1:\n",
        "        seed = random.randint(0, 2**32 - 1)\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "    init_image = Image.fromarray(init_image)\n",
        "    init_image = init_image.resize((width, height), Image.Resampling.NEAREST)\n",
        "\n",
        "    output = pipeline.img2img(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        image=init_image,\n",
        "        strength=strength,\n",
        "        num_inference_steps=steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=generator\n",
        "    )\n",
        "\n",
        "    if not output.nsfw_content_detected[0]:\n",
        "      #print(\"Not NSFW\")\n",
        "      return output.images[0], \"Success\"\n",
        "    else:\n",
        "      #print(\"NSFW Detected\")\n",
        "      return output.images[0], \"NSFW content detected\"\n",
        "\n",
        "def ui():\n",
        "    with gr.Blocks(css=\"\"\"\n",
        "    .absolute-top-right {\n",
        "    position: absolute !important;\n",
        "    top: 27px;\n",
        "    right: 2px;\n",
        "    z-index: 100;\n",
        "    }\n",
        "    .absolute-top-right button {\n",
        "    min-width: 32px !important;\n",
        "    height: 32px !important;\n",
        "    padding: 0 !important;\n",
        "    position: absolute;\n",
        "    left: 4px;\n",
        "    }\n",
        "    \"\"\") as demo:\n",
        "        global pipeline\n",
        "        with gr.Row():\n",
        "            model_dropdown = gr.Dropdown(choices=models, value=models[0], label=\"Model\", interactive=True)\n",
        "            loading_status = gr.Textbox(value=\"Ready\", label=\"Status\", interactive=False)\n",
        "\n",
        "        # load the initial model\n",
        "        pipeline = load_pipeline(links[0])\n",
        "\n",
        "        def on_model_change(model_name):\n",
        "            global pipeline\n",
        "            index = models.index(model_name)\n",
        "            pipeline = load_pipeline(links[index])\n",
        "            print(f\"Model changed to: {model_name}\")\n",
        "            return \"Ready\"\n",
        "\n",
        "        model_dropdown.change(\n",
        "        fn=on_model_change,\n",
        "        inputs=[model_dropdown],\n",
        "        outputs=[loading_status],\n",
        "        queue=False\n",
        "        )\n",
        "\n",
        "        # with gr.Row():\n",
        "        #   txtCustomModel = gr.Textbox(label=\"Custom Model\", placeholder=\"\")\n",
        "        with gr.Tabs():\n",
        "            with gr.Tab(\"Text to Image\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        txt2img_prompt = gr.Textbox(label=\"Prompt\", value=\"a girl in beautiful green valley and blue skies\")\n",
        "                        txt2img_neg_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"nsfw, ugly, noise, blank, blurry, mutation, mangled\")\n",
        "                        txt2img_width = gr.Slider(128, 1024, value=512, step=64, label=\"Width\")\n",
        "                        txt2img_height = gr.Slider(128, 1024, value=512, step=64, label=\"Height\")\n",
        "                        txt2img_steps = gr.Slider(1, 100, value=20, step=1, label=\"Steps\")\n",
        "                        txt2img_guidance = gr.Slider(1, 20, value=7.0, step=0.1, label=\"Guidance Scale\")\n",
        "                        txt2img_seed = gr.Number(value=-1, label=\"Seed\")\n",
        "\n",
        "                    with gr.Column():\n",
        "                        txt2img_generate = gr.Button(\"Generate\", variant=\"primary\")\n",
        "                        txt2img_output = gr.Image(label=\"Output\")\n",
        "                        generation_status = gr.TextArea(label=\"Generation Status\")\n",
        "\n",
        "                txt2img_generate.click(\n",
        "                    fn=txt2img,\n",
        "                    inputs=[\n",
        "                        txt2img_prompt,\n",
        "                        txt2img_neg_prompt,\n",
        "                        txt2img_height,\n",
        "                        txt2img_width,\n",
        "                        txt2img_steps,\n",
        "                        txt2img_guidance,\n",
        "                        txt2img_seed\n",
        "                    ],\n",
        "                    outputs=[txt2img_output,loading_status, generation_status]\n",
        "                )\n",
        "                txt2img_generate.interactive = loading_status.value == \"Ready\"\n",
        "\n",
        "\n",
        "            with gr.Tab(\"Image to Image\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        img2img_input = gr.Image(label=\"Input Image\", type=\"numpy\")\n",
        "                        img2img_prompt = gr.Textbox(label=\"Prompt\", value=\"a girl in beautiful green valley and blue skies\")\n",
        "                        img2img_neg_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"nsfw, ugly, noise, blank, blurry, mutation, mangled\")\n",
        "                        img2img_height = gr.Slider(128, 2048, value=512, step=64, label=\"Height\")\n",
        "                        img2img_width = gr.Slider(128, 2048, value=512, step=64, label=\"Width\")\n",
        "                        img2img_strength = gr.Slider(0, 1, value=0.75, step=0.01, label=\"Strength\")\n",
        "                        img2img_steps = gr.Slider(1, 100, value=20, step=1, label=\"Steps\")\n",
        "                        img2img_guidance = gr.Slider(1, 20, value=7.0, step=0.1, label=\"Guidance Scale\")\n",
        "                        img2img_seed = gr.Number(value=-1, label=\"Seed\")\n",
        "\n",
        "                    with gr.Column():\n",
        "                        img2img_generate = gr.Button(\"Generate\", variant=\"primary\")\n",
        "                        img2img_output = gr.Image(label=\"Output\")\n",
        "                        with gr.Row(elem_classes=\"absolute-top-right\"):  # Position button\n",
        "                            load_to_input = gr.Button(\"⬅️\", scale=0)  # Small arrow button\n",
        "\n",
        "                img2img_generate.click(\n",
        "                    fn=img2img,\n",
        "                    inputs=[\n",
        "                        img2img_input,\n",
        "                        img2img_prompt,\n",
        "                        img2img_neg_prompt,\n",
        "                        img2img_height,\n",
        "                        img2img_width,\n",
        "                        img2img_strength,\n",
        "                        img2img_steps,\n",
        "                        img2img_guidance,\n",
        "                        img2img_seed\n",
        "                    ],\n",
        "                    outputs=[img2img_output,loading_status]\n",
        "                )\n",
        "                img2img_generate.interactive = loading_status.value == \"Ready\"\n",
        "\n",
        "                load_to_input.click(\n",
        "                fn=lambda x: x,\n",
        "                inputs=img2img_output,\n",
        "                outputs=img2img_input\n",
        "                )\n",
        "    return demo\n",
        "\n",
        "demo = ui()\n",
        "# demo.launch(debug=True)\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fahReuTEWTUO"
      },
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "diffusion",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
