{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBByDHe3Su6a",
        "outputId": "e1cb9fc2-260f-47ae-af0c-d1ed265b8a97"
      },
      "outputs": [],
      "source": [
        "# 1. Install the required packages\n",
        "# On Windows, you just need to execute this cell for once.\n",
        "try:\n",
        "    import google.colab\n",
        "    # IN_COLAB = True\n",
        "except ImportError:\n",
        "    # IN_COLAB = False\n",
        "    %pip install -q git+https://github.com/huggingface/transformers\n",
        "    %pip install -q git+https://github.com/huggingface/accelerate\n",
        "\n",
        "%pip install -q git+https://github.com/huggingface/diffusers\n",
        "%pip install -q gradio ftfy tensorboard peft\n",
        "%pip install -q bitsandbytes\n",
        "#%pip install -U git+https://github.com/TimDettmers/bitsandbytes.git\n",
        "%pip install -q xformers --index-url https://download.pytorch.org/whl/cu124\n",
        "#%pip install -U git+https://github.com/facebookresearch/xformers.git@main\n",
        "print(\"Package installation finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gVoVoNmnsrN",
        "outputId": "8b3ee194-64cd-49c9-ef17-2b7852b9fb3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_dreambooth_lora.py already exists, skipping download.\n",
            "convert_diffusers_to_original_stable_diffusion.py already exists, skipping download.\n"
          ]
        }
      ],
      "source": [
        "# 2. Create folders and download training scripts\n",
        "import os, shutil\n",
        "\n",
        "dataset_dir = \"./dataset\"\n",
        "output_dir = \"./output\"\n",
        "logging_dir = \"./logs\"\n",
        "class_dir = \"./class\"\n",
        "\n",
        "token_name = \"sks\"\n",
        "pipeline = None\n",
        "\n",
        "def reset_data():\n",
        "  # Create the directories if they don't exist\n",
        "  os.makedirs(dataset_dir, exist_ok=True)\n",
        "  # Delete the 'output' folder and its contents\n",
        "  shutil.rmtree(output_dir, ignore_errors=True)\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  # Delete the 'log' folder and its contents\n",
        "  shutil.rmtree(logging_dir, ignore_errors=True)\n",
        "  os.makedirs(logging_dir, exist_ok=True)\n",
        "  # Delete the 'class' folder and its contents\n",
        "  # shutil.rmtree(class_dir, ignore_errors=True)\n",
        "  os.makedirs(class_dir, exist_ok=True)\n",
        "  # Delete the 'dataset' folder and its contents\n",
        "  # shutil.rmtree(dataset_dir, ignore_errors=True)\n",
        "  # os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    # fetch train_dreambooth.py if it doesn't exist\n",
        "    if not os.path.exists(\"train_dreambooth_lora.py\"):\n",
        "        !wget https://raw.githubusercontent.com/jomo0825/MrFuGenerativeAI/main/LoRA/train_dreambooth_lora.py\n",
        "    else:\n",
        "        print(\"train_dreambooth_lora.py already exists, skipping download.\")\n",
        "\n",
        "    # fetch convertosdv2.py if it doesn't exist\n",
        "    if not os.path.exists(\"convert_diffusers_to_original_stable_diffusion.py\"):\n",
        "        !wget https://raw.githubusercontent.com/jomo0825/MrFuGenerativeAI/main/utils/convert_diffusers_to_original_stable_diffusion.py\n",
        "    else:\n",
        "        print(\"convert_diffusers_to_original_stable_diffusion.py already exists, skipping download.\")\n",
        "except ImportError:\n",
        "    # fetch train_dreambooth.py if it doesn't exist\n",
        "    if not os.path.exists(\"train_dreambooth_lora.py\"):\n",
        "        !curl -O https://raw.githubusercontent.com/jomo0825/MrFuGenerativeAI/main/LoRA/train_dreambooth_lora.py\n",
        "    else:\n",
        "        print(\"train_dreambooth_lora.py already exists, skipping download.\")\n",
        "\n",
        "    # fetch convertosdv2.py if it doesn't exist\n",
        "    if not os.path.exists(\"convert_diffusers_to_original_stable_diffusion.py\"):\n",
        "        !curl -O https://raw.githubusercontent.com/jomo0825/MrFuGenerativeAI/main/utils/convert_diffusers_to_original_stable_diffusion.py\n",
        "    else:\n",
        "        print(\"convert_diffusers_to_original_stable_diffusion.py already exists, skipping download.\")\n",
        "\n",
        "ipynb_checkpoints = os.path.join( dataset_dir, \".ipynb_checkpoints\")\n",
        "shutil.rmtree(\".gradio\", ignore_errors=True)\n",
        "shutil.rmtree(\".config\", ignore_errors=True)\n",
        "shutil.rmtree(ipynb_checkpoints, ignore_errors=True)\n",
        "\n",
        "reset_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "0wl0d-1HlRJZ",
        "outputId": "dde4be7e-619d-4dd7-8ba7-9c0abe57d899"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A matching Triton is not available, some optimizations will not be enabled\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\anaconda3\\envs\\diffusion\\lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
            "    import triton  # noqa\n",
            "ModuleNotFoundError: No module named 'triton'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "03/28/2025 00:38:12 - INFO - train_dreambooth_lora - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Command: --pretrained_model_name_or_path stable-diffusion-v1-5/stable-diffusion-v1-5 --instance_data_dir ./dataset --instance_prompt sks --output_dir ./output --train_batch_size 1 --resolution 512 --lr_scheduler cosine --learning_rate 0.0001 --lr_warmup_steps 0 --gradient_accumulation_steps 1 --num_validation_images 1 --validation_prompt sks --validation_epochs 10 --max_train_steps 1500 --mixed_precision no --use_8bit_adam --gradient_checkpointing --enable_xformers_memory_efficient_attention --logging_dir ./logs --seed 1 --checkpointing_steps 1501 --rank 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "{'prediction_type', 'timestep_spacing', 'rescale_betas_zero_snr', 'thresholding', 'sample_max_value', 'clip_sample_range', 'dynamic_thresholding_ratio', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Instantiating UNet2DConditionModel model under default dtype torch.float32.\n",
            "{'time_embedding_act_fn', 'timestep_post_act', 'time_embedding_dim', 'reverse_transformer_layers_per_block', 'num_attention_heads', 'transformer_layers_per_block', 'time_cond_proj_dim', 'addition_time_embed_dim', 'use_linear_projection', 'num_class_embeds', 'upcast_attention', 'class_embeddings_concat', 'class_embed_type', 'attention_type', 'encoder_hid_dim_type', 'addition_embed_type', 'dropout', 'projection_class_embeddings_input_dim', 'resnet_time_scale_shift', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'cross_attention_norm', 'conv_out_kernel', 'time_embedding_type', 'conv_in_kernel', 'only_cross_attention', 'resnet_skip_time_act', 'resnet_out_scale_factor', 'dual_cross_attention', 'mid_block_type', 'addition_embed_type_num_heads'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "03/28/2025 00:38:22 - INFO - train_dreambooth_lora - ***** Running training *****\n",
            "03/28/2025 00:38:22 - INFO - train_dreambooth_lora -   Num examples = 7\n",
            "03/28/2025 00:38:22 - INFO - train_dreambooth_lora -   Num batches each epoch = 7\n",
            "03/28/2025 00:38:22 - INFO - train_dreambooth_lora -   Num Epochs = 215\n",
            "03/28/2025 00:38:22 - INFO - train_dreambooth_lora -   Instantaneous batch size per device = 1\n",
            "03/28/2025 00:38:22 - INFO - train_dreambooth_lora -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "03/28/2025 00:38:22 - INFO - train_dreambooth_lora -   Gradient Accumulation steps = 1\n",
            "03/28/2025 00:38:22 - INFO - train_dreambooth_lora -   Total optimization steps = 1500\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ac06bb0b1a941ef89fb32c0ebc1a8d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Steps:   0%|          | 0/1500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c5ca3055ee746ca92b6ca6a092623c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:38:31 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c47748b95dfc4f3c9d27c5e9ff1052e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:39:41 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12d10904999f405b9e68533c0412e847",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:40:50 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "147/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5314a786acd0480a9a7dd50d17080f44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:41:58 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "217/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf00eefe60ed4597b20cdcb71015181b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:43:07 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "287/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08e4beb6d05e4a48ac884e8d3dae2a6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:44:16 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "357/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d27140d911a42cfbf321f34e7e64735",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:45:25 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "427/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e8a35d5c7324550a0a2e1dc50f3e235",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:46:34 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "497/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "570c8e4fe0d442fb8b02817e015b490b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:47:43 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "567/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26c631d46734458bbff4bafff6562cbe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:48:52 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "637/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eebece8582c4765b6657fa827b8a350",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:50:00 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "707/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e6340b015f645dcb8e1e022db9f7547",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:51:08 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "777/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bfac73399ca43069fd8bad824ebfd5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at C:\\Users\\NTUT_mrfu\\.cache\\huggingface\\hub\\models--stable-diffusion-v1-5--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "03/28/2025 00:52:16 - INFO - train_dreambooth_lora - Running validation... \n",
            " Generating 1 images with prompt: sks.\n",
            "{'prediction_type', 'use_exponential_sigmas', 'solver_type', 'solver_order', 'thresholding', 'use_flow_sigmas', 'euler_at_final', 'dynamic_thresholding_ratio', 'lower_order_final', 'lambda_min_clipped', 'rescale_betas_zero_snr', 'use_lu_lambdas', 'use_karras_sigmas', 'algorithm_type', 'use_beta_sigmas', 'sample_max_value', 'final_sigmas_type', 'variance_type', 'timestep_spacing', 'flow_shift'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "847/1500\n"
          ]
        }
      ],
      "source": [
        "# 3. Create a WebUI for LoRA Dreambooth training\n",
        "# It will download the SD v1.5 for the 1st time training\n",
        "# A very good reference:\n",
        "# https://www.reddit.com/r/StableDiffusion/comments/ybxv7h/good_dreambooth_formula/\n",
        "\n",
        "import gradio as gr\n",
        "import sys\n",
        "import threading\n",
        "from train_dreambooth_lora import main as train_dreambooth_lora\n",
        "from train_dreambooth_lora import parse_args\n",
        "import time, os, logging, shutil\n",
        "from os import path\n",
        "import subprocess\n",
        "import shlex\n",
        "import queue\n",
        "from PIL import Image\n",
        "\n",
        "def parse_lr_schedule(lr_schedule_str):\n",
        "    schedule = []\n",
        "    segments = lr_schedule_str.split(',')\n",
        "    for segment in segments:\n",
        "        if ':' in segment:\n",
        "            lr, steps = segment.split(':')\n",
        "            schedule.append((float(lr), int(steps)))\n",
        "        else:\n",
        "            schedule.append((float(segment), None))  # Final constant learning rate\n",
        "    return schedule\n",
        "\n",
        "def get_learning_rate_at_step(lr_schedule, step):\n",
        "    current_step = 0\n",
        "    for lr, segment_steps in lr_schedule:\n",
        "        if segment_steps is None or step < current_step + segment_steps:\n",
        "            return lr\n",
        "        current_step += segment_steps\n",
        "    return lr_schedule[-1][0]  # Return the last LR if beyond defined steps\n",
        "\n",
        "# Callback to update the preview image in the UI\n",
        "def preview_callback(image, step):\n",
        "    global current_preview, current_status, max_train_steps, current_step\n",
        "    current_step = step\n",
        "    current_preview = image\n",
        "    print(f\"{step}/{max_train_steps}\")\n",
        "\n",
        "def stop_training():\n",
        "    global stop_flag\n",
        "    stop_flag.set()\n",
        "    return gr.update(value=\"Training will be stopped...Waiting for the final preview...\")\n",
        "\n",
        "def run_training(prompt, instance_prompt, class_prompt, num_training_steps,\n",
        "                learning_rate, batch_size, preview_save_steps, preview_seed, rank):\n",
        "    global current_preview, current_status, max_train_steps, token_name\n",
        "    global current_step, finish_event, stop_flag, pipeline\n",
        "    current_preview = None  # Reset the preview\n",
        "    current_status = \"Training started...\"  # Initial status\n",
        "    token_name = instance_prompt\n",
        "    pipeline = None\n",
        "\n",
        "    # Define LoRA DreamBooth training parameters as a list of command-line arguments.\n",
        "    command = [\n",
        "        \"--pretrained_model_name_or_path\", \"stable-diffusion-v1-5/stable-diffusion-v1-5\",  # or your chosen model\n",
        "        \"--instance_data_dir\", dataset_dir,  # folder with your subject images\n",
        "        \"--instance_prompt\", instance_prompt,  # prompt identifier for your subject\n",
        "        \"--output_dir\", output_dir,          # where to save your LoRA model\n",
        "        \"--train_batch_size\", str(batch_size),\n",
        "        \"--resolution\", \"512\",\n",
        "        \"--lr_scheduler\", \"cosine\",\n",
        "        \"--learning_rate\", str(learning_rate),\n",
        "        \"--lr_warmup_steps\", \"0\",\n",
        "        \"--gradient_accumulation_steps\", \"1\",\n",
        "        \"--num_validation_images\", \"1\",\n",
        "        \"--validation_prompt\", prompt,\n",
        "        \"--validation_steps\", str(preview_save_steps),\n",
        "        \"--max_train_steps\", str(num_training_steps),\n",
        "        \"--mixed_precision\", \"no\",\n",
        "        \"--use_8bit_adam\",\n",
        "        \"--gradient_checkpointing\",\n",
        "        \"--enable_xformers_memory_efficient_attention\",\n",
        "        \"--logging_dir\", logging_dir,\n",
        "        \"--seed\", str(preview_seed),\n",
        "        \"--checkpointing_steps\", str(num_training_steps+1),\n",
        "        \"--rank\", str(rank),  # LoRA rank parameter\n",
        "\n",
        "        # \"--class_data_dir\", class_dir,        # folder with class images (for prior preservation)\n",
        "        # \"--class_prompt\", class_prompt,        # prompt for class images\n",
        "        # \"--with_prior_preservation\",           # enable prior preservation if you have class images\n",
        "        # \"--num_class_images\", \"10\",           # adjust based on your available class images\n",
        "    ]\n",
        "\n",
        "    args = parse_args(command)\n",
        "    max_train_steps = args.max_train_steps\n",
        "\n",
        "    # Print the command for debugging\n",
        "    print(\"Command:\", \" \".join(command))\n",
        "\n",
        "    # Disable logging\n",
        "    logging.getLogger(\"accelerate\").disabled = True\n",
        "\n",
        "    def worker(finish_event, stop_flag):\n",
        "        try:\n",
        "            train_dreambooth_lora(args, {\"_callback\": preview_callback, \"stop_flag\": stop_flag})\n",
        "        except Exception as e:\n",
        "            print(f\"Training error: {e}\")\n",
        "        finally:\n",
        "            finish_event.set()\n",
        "\n",
        "    finish_event = threading.Event()\n",
        "    stop_flag = threading.Event()\n",
        "    finish_event.clear()\n",
        "    stop_flag.clear()\n",
        "\n",
        "    train_thread = threading.Thread(target=worker, args=(finish_event, stop_flag))\n",
        "    train_thread.start()\n",
        "    yield gr.update(value=None), gr.update(value=f\"Training started.\")\n",
        "\n",
        "    while not finish_event.is_set():\n",
        "        if current_preview is not None:\n",
        "            yield gr.update(value=current_preview), gr.update(value=f\"Preview at {current_step} step.\")\n",
        "            current_preview = None\n",
        "        time.sleep(1)\n",
        "\n",
        "    train_thread.join()\n",
        "\n",
        "    # Update status when training completes\n",
        "    current_status = \"Saving LoRA model...\"\n",
        "    yield gr.update(value=current_preview), gr.update(value=current_status)\n",
        "    pipeline = None\n",
        "\n",
        "    # Save the model in a format compatible with original Stable Diffusion\n",
        "    try:\n",
        "        # subprocess.run([\"python\", \"convert_lora_to_safetensors.py\", \"--model_path\", output_dir, \"--checkpoint_path\", \"lora_model.safetensors\"], check=True)\n",
        "        current_status = \"LoRA model saved successfully!\"\n",
        "    except Exception as e:\n",
        "        current_status = f\"Error saving model: {e}\"\n",
        "\n",
        "    yield gr.update(value=current_preview), gr.update(value=current_status)\n",
        "\n",
        "# Define a function to move the selected file\n",
        "def copy_file(target_directory, file_path):\n",
        "    if file_path is not None:\n",
        "        filename = os.path.basename(file_path)\n",
        "        destination_path = os.path.join(target_directory, filename)\n",
        "        # test if the destination_path already exists\n",
        "        if not os.path.exists(destination_path):\n",
        "            shutil.copy(file_path, destination_path)\n",
        "            print(f\"Copy {filename} to {target_directory}\")\n",
        "        else:\n",
        "            print(f\"{filename} already exists.\")\n",
        "    else:\n",
        "        print(\"No file selected.\")\n",
        "\n",
        "def process_files(files):\n",
        "    file_info = []\n",
        "    \n",
        "    # Make sure dataset directory exists\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    for file in files:\n",
        "        fileBasename = os.path.basename(file)\n",
        "        destination_path = os.path.join(dataset_dir, fileBasename)\n",
        "        if not os.path.exists(destination_path):\n",
        "            file_info.append(f\"File: {file.name}\")\n",
        "            copy_file(dataset_dir, file.name)\n",
        "    return f\"{len(file_info)} files uploaded.\"\n",
        "\n",
        "def process_class_files(files):\n",
        "    file_info = []\n",
        "    \n",
        "    # Make sure class directory exists\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "    for file in files:\n",
        "        fileBasename = os.path.basename(file)\n",
        "        destination_path = os.path.join(class_dir, fileBasename)\n",
        "        if not os.path.exists(destination_path):\n",
        "            file_info.append(f\"File: {file.name}\")\n",
        "            copy_file(class_dir, file.name)\n",
        "    return f\"{len(file_info)} files uploaded.\"\n",
        "\n",
        "def delete_dataset():\n",
        "    shutil.rmtree(dataset_dir, ignore_errors=True)\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "    return \"Dataset deleted.\"\n",
        "\n",
        "def delete_class_images():\n",
        "    shutil.rmtree(class_dir, ignore_errors=True)\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    return \"Class images deleted.\"\n",
        "\n",
        "def reset_data():\n",
        "    delete_dataset()\n",
        "    delete_class_images()\n",
        "    return \"All data reset.\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "os.makedirs(class_dir, exist_ok=True)\n",
        "os.makedirs(logging_dir, exist_ok=True)\n",
        "\n",
        "def ui():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# Stable Diffusion LoRA Dreambooth WebUI\")\n",
        "        gr.Markdown(\"Train Stable Diffusion model with LoRA for faster fine-tuning and lower memory usage.\")\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                with gr.Row():\n",
        "                    file_input = gr.File(file_count=\"multiple\", label=\"Upload Dataset\",\n",
        "                        height=200, file_types=[\"image\"])\n",
        "                with gr.Row():\n",
        "                    submit_btn = gr.Button(\"Upload Dataset\")\n",
        "                    delete_data_btn = gr.Button(\"Delete Dataset\")\n",
        "                with gr.Row():\n",
        "                    output = gr.Textbox(label=\"Results\")\n",
        "                submit_btn.click(fn=process_files, inputs=file_input, outputs=output)\n",
        "                delete_data_btn.click(fn=delete_dataset, outputs=output)\n",
        "            with gr.Column():\n",
        "                with gr.Row():\n",
        "                    class_file_input = gr.File(file_count=\"multiple\", label=\"Upload Class Images\",\n",
        "                        height=200, file_types=[\"image\"])\n",
        "                with gr.Row():\n",
        "                    class_submit_btn = gr.Button(\"Upload Class Images\")\n",
        "                    class_delete_data_btn = gr.Button(\"Delete Class Images\")\n",
        "                with gr.Row():\n",
        "                    class_output = gr.Textbox(label=\"Results\")\n",
        "                class_submit_btn.click(fn=process_class_files, inputs=class_file_input, outputs=class_output)\n",
        "                class_delete_data_btn.click(fn=delete_class_images, outputs=class_output)\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1, min_width=300):\n",
        "                instance_prompt = gr.Textbox(label=\"Instance Prompt (your subject)\", \n",
        "                    placeholder=\"a photo of sks person\", interactive=True)\n",
        "                class_prompt = gr.Textbox(label=\"Class Prompt\", \n",
        "                    placeholder=\"a photo of person\", interactive=True)\n",
        "                prompt = gr.Textbox(label=\"Preview Prompt\", \n",
        "                    placeholder=\"a photo of sks person on the beach\", interactive=True)\n",
        "                num_training_steps = gr.Number(label=\"Number of Training Steps\", value=1500, interactive=True)\n",
        "                learning_rate = gr.Number(label=\"Learning Rate\", value=0.0001, interactive=True)\n",
        "                batch_size = gr.Number(label=\"Batch Size\", value=1, interactive=True)\n",
        "                preview_save_steps = gr.Number(label=\"Preview Steps\", value=50, interactive=True)\n",
        "                preview_seed = gr.Number(label=\"Preview Seed\", value=1, interactive=True)\n",
        "                rank = gr.Slider(label=\"LoRA Rank\", minimum=1, maximum=128, value=32, step=1, \n",
        "                    info=\"Higher rank = more capacity but larger file size\")\n",
        "            with gr.Column(scale=1, min_width=300):\n",
        "                output_image = gr.Image(label=\"Generated Image\")\n",
        "                generate_status = gr.Textbox(value=\"Status messages will appear here.\", label=\"Status\", interactive=False)\n",
        "                generate_button = gr.Button(\"Start Training\")\n",
        "                cancel_button = gr.Button(\"Cancel Training\")\n",
        "                reset_button = gr.Button(\"Reset Data\")\n",
        "\n",
        "        generate_button.click(\n",
        "            fn=run_training,\n",
        "            inputs=[prompt, instance_prompt, class_prompt, num_training_steps,\n",
        "                    learning_rate, batch_size, preview_save_steps, preview_seed, rank],\n",
        "            outputs=[output_image, generate_status],\n",
        "            show_progress=True,\n",
        "            queue=True\n",
        "        )\n",
        "\n",
        "        cancel_button.click(\n",
        "            fn=stop_training,\n",
        "            outputs=[generate_status]\n",
        "        )\n",
        "\n",
        "        reset_button.click(\n",
        "            fn=reset_data,\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Initialize global variables\n",
        "current_preview = None\n",
        "current_status = \"Ready\"\n",
        "max_train_steps = 0\n",
        "current_step = 0\n",
        "token_name = \"\"\n",
        "finish_event = None\n",
        "stop_flag = None\n",
        "pipeline = None\n",
        "\n",
        "# Launch the UI\n",
        "demo = ui()\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg-WlvSVsA_g"
      },
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6owcy6kvIj44"
      },
      "outputs": [],
      "source": [
        "# Optional: Loads the logs in TensorBoard\n",
        "# If you are using Windows, open http://localhost:8888 in browser\n",
        "# Enable auto update for each 30 seconds, look into Image tab and wait for update.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=logs/ --host localhost --port 8888"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561,
          "referenced_widgets": [
            "664c191b2700474aab27c9dbbcf4275f",
            "7661204b60ea433180ec75b5b4a9af0c",
            "4ecdeb91f9a944e2ab2ac3a46affa1bc",
            "bda2c918187a4f3eaaf8660f28485ecd",
            "218051f498db4d51929aef8134d3f163",
            "b16fcf3d9c554a8da06638d502a23b1c",
            "fa5e6a62a91649ff890753c91aee68e0",
            "5f96d6c37c0d489c9549574b8f3ce16d",
            "78dae63099f4414c8708dffc2289b155",
            "ca315520908a4e829f1f3ae3d36825a4",
            "849dea0d5df3413d8b6cc415912d16bd"
          ]
        },
        "id": "4ZF3ivnHN3d7",
        "outputId": "a2d11f21-5899-4906-a8a7-e1fbcd783d91"
      },
      "outputs": [],
      "source": [
        "# Optional: Test the Dreambooth model\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "preview_prompt = f\"{token_name}, a illustration of wpg. cyan, reflective, flower, 8k, lineart, extremly detailed eyes, digital painting. masterpiece, best quality.\"\n",
        "negative_prompt = \"grain, pattern, disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\n",
        "\n",
        "if pipeline is None:\n",
        "  pipeline = StableDiffusionPipeline.from_single_file(\n",
        "      \"model.safetensors\",\n",
        "      torch_dtype=torch.float16,\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "output = pipeline(\n",
        "    preview_prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    num_inference_steps=30,\n",
        "    guidance_scale=7,\n",
        ")\n",
        "\n",
        "display(output.images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRT9Vuo9bdq0",
        "outputId": "673bf10e-da57-4ea2-f1bd-e3fbbb25f101"
      },
      "outputs": [],
      "source": [
        "# 4. Download your model.savetensors\n",
        "# If you are using Colab, you can mount Google Drive and upload your model.safetensors\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Create a directory in Google Drive if it doesn't exist\n",
        "    import os\n",
        "    target_dir = \"/content/drive/MyDrive/Dreambooth\"\n",
        "    if not os.path.exists(target_dir):\n",
        "        os.makedirs(target_dir)\n",
        "        print(f\"Created directory: {target_dir}\")\n",
        "    else:\n",
        "        print(f\"Directory already exists: {target_dir}\")\n",
        "\n",
        "    # Copy your file to Drive\n",
        "    !cp /content/model.safetensors {target_dir}/{token_name}.safetensors\n",
        "    print(f\"Your Dreambooth model has been uploaded to your Google Drive folder {target_dir}\")\n",
        "except:\n",
        "  pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "diffusion",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "218051f498db4d51929aef8134d3f163": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ecdeb91f9a944e2ab2ac3a46affa1bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f96d6c37c0d489c9549574b8f3ce16d",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78dae63099f4414c8708dffc2289b155",
            "value": 30
          }
        },
        "5f96d6c37c0d489c9549574b8f3ce16d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "664c191b2700474aab27c9dbbcf4275f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7661204b60ea433180ec75b5b4a9af0c",
              "IPY_MODEL_4ecdeb91f9a944e2ab2ac3a46affa1bc",
              "IPY_MODEL_bda2c918187a4f3eaaf8660f28485ecd"
            ],
            "layout": "IPY_MODEL_218051f498db4d51929aef8134d3f163"
          }
        },
        "7661204b60ea433180ec75b5b4a9af0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b16fcf3d9c554a8da06638d502a23b1c",
            "placeholder": "​",
            "style": "IPY_MODEL_fa5e6a62a91649ff890753c91aee68e0",
            "value": "100%"
          }
        },
        "78dae63099f4414c8708dffc2289b155": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "849dea0d5df3413d8b6cc415912d16bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b16fcf3d9c554a8da06638d502a23b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bda2c918187a4f3eaaf8660f28485ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca315520908a4e829f1f3ae3d36825a4",
            "placeholder": "​",
            "style": "IPY_MODEL_849dea0d5df3413d8b6cc415912d16bd",
            "value": " 30/30 [00:04&lt;00:00,  6.72it/s]"
          }
        },
        "ca315520908a4e829f1f3ae3d36825a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa5e6a62a91649ff890753c91aee68e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
